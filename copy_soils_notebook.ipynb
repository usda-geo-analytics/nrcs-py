{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output of get_mothership_dirs() (var mothership_dirs):\n",
      "\tKey: download, value: C:\\Users\\misti.wudtke\\OneDrive - USDA\\PYTHON\\NRCSPY\\copy_soils\\working_dir\\_F\\data\\download\n",
      "\tKey: .mdb, value: C:\\Users\\misti.wudtke\\OneDrive - USDA\\PYTHON\\NRCSPY\\copy_soils\\working_dir\\_F\\data\\FOTG\\Section_II\\FY24\n",
      "\tKey: .shp, value: C:\\Users\\misti.wudtke\\OneDrive - USDA\\PYTHON\\NRCSPY\\copy_soils\\working_dir\\_F\\data\\geodata\\soils\n",
      "\n",
      "Output of get_filepaths (var filepaths):\n",
      "\tKey: .mdb, Values:\n",
      "\t\tSubkey: or003, Values:\n",
      "\t\t\tC:\\Users\\misti.wudtke\\OneDrive - USDA\\PYTHON\\NRCSPY\\copy_soils\\working_dir\\_F\\data\\FOTG\\Section_II\\FY24\\soil_d_or003_FY24.mdb\n",
      "\t\tSubkey: or007, Values:\n",
      "\t\t\tC:\\Users\\misti.wudtke\\OneDrive - USDA\\PYTHON\\NRCSPY\\copy_soils\\working_dir\\_F\\data\\FOTG\\Section_II\\FY24\\soil_d_or007_FY24.mdb\n",
      "\t\tSubkey: or009, Values:\n",
      "\t\t\tC:\\Users\\misti.wudtke\\OneDrive - USDA\\PYTHON\\NRCSPY\\copy_soils\\working_dir\\_F\\data\\FOTG\\Section_II\\FY24\\soil_d_or009_FY24.mdb\n",
      "\tKey: .shp, Values:\n",
      "\t\tSubkey: or003, Values:\n",
      "\t\t\tC:\\Users\\misti.wudtke\\OneDrive - USDA\\PYTHON\\NRCSPY\\copy_soils\\working_dir\\_F\\data\\geodata\\soils\\soilmu_a_or003_202409.cpg\n",
      "\t\t\tC:\\Users\\misti.wudtke\\OneDrive - USDA\\PYTHON\\NRCSPY\\copy_soils\\working_dir\\_F\\data\\geodata\\soils\\soilmu_a_or003_202409.dbf\n",
      "\t\t\tC:\\Users\\misti.wudtke\\OneDrive - USDA\\PYTHON\\NRCSPY\\copy_soils\\working_dir\\_F\\data\\geodata\\soils\\soilmu_a_or003_202409.prj\n",
      "\t\t\tC:\\Users\\misti.wudtke\\OneDrive - USDA\\PYTHON\\NRCSPY\\copy_soils\\working_dir\\_F\\data\\geodata\\soils\\soilmu_a_or003_202409.shp\n",
      "\t\t\tC:\\Users\\misti.wudtke\\OneDrive - USDA\\PYTHON\\NRCSPY\\copy_soils\\working_dir\\_F\\data\\geodata\\soils\\soilmu_a_or003_202409.shp.xml\n",
      "\t\t\tC:\\Users\\misti.wudtke\\OneDrive - USDA\\PYTHON\\NRCSPY\\copy_soils\\working_dir\\_F\\data\\geodata\\soils\\soilmu_a_or003_202409.shx\n",
      "\t\tSubkey: or007, Values:\n",
      "\t\t\tC:\\Users\\misti.wudtke\\OneDrive - USDA\\PYTHON\\NRCSPY\\copy_soils\\working_dir\\_F\\data\\geodata\\soils\\soilmu_a_or007_202409.cpg\n",
      "\t\t\tC:\\Users\\misti.wudtke\\OneDrive - USDA\\PYTHON\\NRCSPY\\copy_soils\\working_dir\\_F\\data\\geodata\\soils\\soilmu_a_or007_202409.dbf\n",
      "\t\t\tC:\\Users\\misti.wudtke\\OneDrive - USDA\\PYTHON\\NRCSPY\\copy_soils\\working_dir\\_F\\data\\geodata\\soils\\soilmu_a_or007_202409.prj\n",
      "\t\t\tC:\\Users\\misti.wudtke\\OneDrive - USDA\\PYTHON\\NRCSPY\\copy_soils\\working_dir\\_F\\data\\geodata\\soils\\soilmu_a_or007_202409.shp\n",
      "\t\t\tC:\\Users\\misti.wudtke\\OneDrive - USDA\\PYTHON\\NRCSPY\\copy_soils\\working_dir\\_F\\data\\geodata\\soils\\soilmu_a_or007_202409.shp.xml\n",
      "\t\t\tC:\\Users\\misti.wudtke\\OneDrive - USDA\\PYTHON\\NRCSPY\\copy_soils\\working_dir\\_F\\data\\geodata\\soils\\soilmu_a_or007_202409.shx\n",
      "\t\tSubkey: or009, Values:\n",
      "\t\t\tC:\\Users\\misti.wudtke\\OneDrive - USDA\\PYTHON\\NRCSPY\\copy_soils\\working_dir\\_F\\data\\geodata\\soils\\soilmu_a_or009_202409.cpg\n",
      "\t\t\tC:\\Users\\misti.wudtke\\OneDrive - USDA\\PYTHON\\NRCSPY\\copy_soils\\working_dir\\_F\\data\\geodata\\soils\\soilmu_a_or009_202409.dbf\n",
      "\t\t\tC:\\Users\\misti.wudtke\\OneDrive - USDA\\PYTHON\\NRCSPY\\copy_soils\\working_dir\\_F\\data\\geodata\\soils\\soilmu_a_or009_202409.prj\n",
      "\t\t\tC:\\Users\\misti.wudtke\\OneDrive - USDA\\PYTHON\\NRCSPY\\copy_soils\\working_dir\\_F\\data\\geodata\\soils\\soilmu_a_or009_202409.shp\n",
      "\t\t\tC:\\Users\\misti.wudtke\\OneDrive - USDA\\PYTHON\\NRCSPY\\copy_soils\\working_dir\\_F\\data\\geodata\\soils\\soilmu_a_or009_202409.shp.xml\n",
      "\t\t\tC:\\Users\\misti.wudtke\\OneDrive - USDA\\PYTHON\\NRCSPY\\copy_soils\\working_dir\\_F\\data\\geodata\\soils\\soilmu_a_or009_202409.shx\n",
      "\n",
      "Output of get_satellites (satellite_list):\n",
      "\t_field_office_1\n",
      "\t_field_office_2\n",
      "\t_field_office_3\n",
      "\n",
      "Output of get_satellite_dirs(_field_office_1):\n",
      "\tKey: .mdb, value: C:\\Users\\misti.wudtke\\OneDrive - USDA\\PYTHON\\NRCSPY\\copy_soils\\working_dir\\_field_office_1\\data\\FOTG\\Section_II\n",
      "\tKey: .shp, value: C:\\Users\\misti.wudtke\\OneDrive - USDA\\PYTHON\\NRCSPY\\copy_soils\\working_dir\\_field_office_1\\data\\geodata\\soils\n",
      "\n",
      "Output of get_sat_required _field_office_1:\n",
      "\tKey: .mdb, value: {'or003'}\n",
      "\tKey: .shp, value: {'or003'}\n",
      "\n",
      "Output of get_satellite_dirs(_field_office_2):\n",
      "\tKey: .mdb, value: C:\\Users\\misti.wudtke\\OneDrive - USDA\\PYTHON\\NRCSPY\\copy_soils\\working_dir\\_field_office_2\\data\\FOTG\\Section_II\n",
      "\tKey: .shp, value: C:\\Users\\misti.wudtke\\OneDrive - USDA\\PYTHON\\NRCSPY\\copy_soils\\working_dir\\_field_office_2\\data\\geodata\\soils\n",
      "\n",
      "Output of get_sat_required _field_office_2:\n",
      "\tKey: .mdb, value: {'or007'}\n",
      "\tKey: .shp, value: {'or007'}\n",
      "\n",
      "Output of get_satellite_dirs(_field_office_3):\n",
      "\tKey: .mdb, value: C:\\Users\\misti.wudtke\\OneDrive - USDA\\PYTHON\\NRCSPY\\copy_soils\\working_dir\\_field_office_3\\data\\FOTG\\Section_II\n",
      "\tKey: .shp, value: C:\\Users\\misti.wudtke\\OneDrive - USDA\\PYTHON\\NRCSPY\\copy_soils\\working_dir\\_field_office_3\\data\\geodata\\soils\n",
      "\n",
      "Output of get_sat_required _field_office_3:\n",
      "\tKey: .mdb, value: {'or009'}\n",
      "\tKey: .shp, value: {'or009'}\n"
     ]
    }
   ],
   "source": [
    "# Really I will fill in the important comments later\n",
    "\n",
    "# Comments on server naming conventions and what we can assume:\n",
    "# State office server_name: aioorpo23fp1\n",
    "# aio | or | po2 | 3fp1\n",
    "# Prefix (always the same): server_name[:3] = \"aio\"\n",
    "# State abbr: server_name[3:5] = \"or\"\n",
    "# Office abbr, 3 digits: server_name[5:8] = \"po2\" (Portland office 2; there are 3 total)\n",
    "# Random last 4 digits (/what does it mean??/): server_name[8:] = \"3fp1\"\n",
    "    # Alt: server_name[-3:] = \"3fp1\"\n",
    "\n",
    "\n",
    "########## ########## ########## ########## ########## ########## \n",
    "# IMPORTS\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import shutil\n",
    "import arcpy\n",
    "\n",
    "\n",
    "########## ########## ########## ########## ########## ########## \n",
    "# INPUT PARAMETERS\n",
    "# These could be exposed in e.g. python toolbox\n",
    "\n",
    "# Switch to control whether this is run on UNCs or letter dr\n",
    "# (Construction of path for each is slightly different)\n",
    "test_local = True\n",
    "\n",
    "if test_local:\n",
    "    # For testing - local letter drive alt mothership source\n",
    "    mothership = r\"C:\\Users\\misti.wudtke\\OneDrive - USDA\\PYTHON\\NRCSPY\\copy_soils\\working_dir\\_F\"\n",
    "    satellite_source = os.path.dirname(mothership)\n",
    "\n",
    "else:\n",
    "    # Non-testing mode assumes data is accessed via UNC \\\\host\\share\n",
    "    mothership = \"aioorpo23fp1\"\n",
    "    satellite_source = \"table_source\" # REPLACE!!!!!!!!\n",
    "\n",
    "# FY stamp to append to MDBs\n",
    "fy_stamp = \"_FY24\"\n",
    "\n",
    "\n",
    "\n",
    "########## ########## ########## ########## ########## ########## \n",
    "# NON-INPUT PARAMETERS\n",
    "\n",
    "# Provides datestamp string in the format of \"_YYYYMMDD\", ready to append\n",
    "date_stamp = datetime.datetime.now().strftime(\"_%Y%m\")\n",
    "\n",
    "# To avoid hard-coded strings everywhere\n",
    "mdb = \".mdb\"\n",
    "shp = \".shp\"\n",
    "\n",
    "prefixes = {mdb: \"soil_d_\", shp: \"soilmu_a_\"}\n",
    "\n",
    "\n",
    "########## ########## ########## ########## ########## ########## \n",
    "# FUNCTIONS\n",
    "\n",
    "def get_root(field_office):\n",
    "\n",
    "    if test_local:\n",
    "        root = Path(satellite_source, field_office, \"data\")\n",
    "\n",
    "    else:\n",
    "        root = f\"\\\\\\\\{field_office}\\\\data\"\n",
    "\n",
    "    return root\n",
    "\n",
    "\n",
    "\n",
    "########## ########## ########## \n",
    "\n",
    "def get_mothership_dirs():\n",
    "\n",
    "    download_dir = Path(get_root(mothership), \"download\")\n",
    "    \n",
    "    # Path to intermediate mdb testination\n",
    "    # (after flattening, before doling to FOs)\n",
    "    mdb_dir = Path(get_root(mothership), \"FOTG\", \"Section_II\", \"FY24\")\n",
    "\n",
    "    # shps don't need flattening, so this is their original location\n",
    "    shp_dir = Path(get_root(mothership), \"geodata\", \"soils\")\n",
    "\n",
    "    mothership_dirs = {\"download\": download_dir, mdb: mdb_dir, shp: shp_dir}\n",
    "\n",
    "    # Print All The Things to make sure our output is what we expect\n",
    "    print(\"\\nOutput of get_mothership_dirs() (var mothership_dirs):\")\n",
    "    for k, v in mothership_dirs.items():\n",
    "        print(f\"\\tKey: {k}, value: {v}\")\n",
    "\n",
    "    return mothership_dirs\n",
    "\n",
    "\n",
    "########## ########## ########## \n",
    "\n",
    "# Walk the dir where all the mdbs are scattered among various subfolders\n",
    "# and consolidate them into one dir\n",
    "def consolidate_mdbs(mothership_dirs):\n",
    "\n",
    "    # Make the new FY dir if it does not already exist\n",
    "    os.makedirs(mothership_dirs[mdb], exist_ok= True)\n",
    "\n",
    "    # [0] is the downloads dir\n",
    "    for root, dirs, files in os.walk(mothership_dirs[\"download\"]):\n",
    "\n",
    "        # Can't iterate files that aren't there\n",
    "        if not files:\n",
    "            continue\n",
    "\n",
    "        # Otherwise (if there are files), iterate\n",
    "        for fi in files:\n",
    "\n",
    "            if fi.endswith(mdb) and fi.startswith(prefixes[mdb]):\n",
    "\n",
    "                # Full path to the file I want to consolidate\n",
    "                from_path = Path(root, fi)\n",
    "\n",
    "                to_path = Path(mothership_dirs[mdb], f\"{fy_stamp}.\".join(fi.rsplit(\".\", 1)))\n",
    "\n",
    "                # Do the copy thing\n",
    "                try:\n",
    "                    shutil.copy(from_path, to_path)\n",
    "\n",
    "                # YES I KNOW...will expand later\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "\n",
    "\n",
    "########## ########## ########## \n",
    "\n",
    "def get_filepaths(mothership_dirs):\n",
    "\n",
    "    # Initialize a dict of dicts\n",
    "    filepaths = {mdb: {}, shp: {}}\n",
    "\n",
    "    for k, v in mothership_dirs.items():\n",
    "\n",
    "        # We don't need to get full filepaths for the original downloads dir\n",
    "        if k == \"download\":\n",
    "            continue\n",
    "\n",
    "        # I don't need to do anything more with the mdbs since they've \n",
    "        # already been FY-stamped when they were consolidated\n",
    "        elif k == mdb:\n",
    "\n",
    "            # Make a list of the full paths of the mdb files to be copied\n",
    "            prepaths = [str(Path(v, f)) for f in os.listdir(v) if f.endswith(k)]\n",
    "\n",
    "            # Now I need that 5-char code within these preprepaths to use\n",
    "            # as the key to the list of all the filepaths\n",
    "            for p in prepaths:\n",
    "\n",
    "                # Chop it at \"soil_d_\"\n",
    "                chop = p.split(prefixes[mdb])\n",
    "\n",
    "                # If it had \"soil_d_\" in it...\n",
    "                if len(chop) > 1:\n",
    "\n",
    "                    # ...Stuff the full filepath in a list as a value in a dict\n",
    "                    # where the 5-char code is the key; this facilitates easy lookup later\n",
    "                    filepaths[k][chop[1][:5]] = [p]\n",
    "\n",
    "        elif k == shp:\n",
    "\n",
    "            # NOTE: THIS GETS ALL THE FILES IN THE DIRECTORY...\n",
    "            preprepaths = [str(Path(v, f)) for f in os.listdir(v) if prefixes[shp] in f]\n",
    "\n",
    "            prepaths = []\n",
    "\n",
    "            # But for shps we still need to apply our datestamp...\n",
    "            for p in preprepaths:\n",
    "\n",
    "                # If the file hasn't already been datestamped, stamp it\n",
    "                if not date_stamp in p:\n",
    "\n",
    "                    # Of course the xml file has to be the special snowflake\n",
    "                    # with two periods and two go#dam# extensions...\n",
    "                    if p.endswith(\".xml\"):\n",
    "                        \n",
    "                        # The \"fake\" period in the xml always preceeds .shp, sooo:\n",
    "                        dated = f\"{date_stamp}.shp\".join(p.rsplit(shp, 1))\n",
    "\n",
    "                    else:\n",
    "                        # Chop up the path, add in the date stamp and zip it back up again\n",
    "                        # This of course just builds the correct string...\n",
    "                        dated = f\"{date_stamp}.\".join(p.rsplit(\".\", 1))\n",
    "\n",
    "                    # ...Can't forget to actually rename the file itself\n",
    "                    os.replace(p, dated)\n",
    "\n",
    "                # Otherwise if the file already has the datestamp, it's all good\n",
    "                else:\n",
    "                    dated = p\n",
    "\n",
    "                prepaths.append(dated)\n",
    "            \n",
    "            for p in prepaths:\n",
    "\n",
    "                # Chop it at \"soilmu_a_\"\n",
    "                chop = p.split(prefixes[shp])\n",
    "\n",
    "                # If it had \"soilmu_a_\" in it...\n",
    "                if len(chop) > 1:\n",
    "\n",
    "                    # Assign 5-char code to var\n",
    "                    # since there's way too much crap going on here\n",
    "                    key_code = chop[1][:5]\n",
    "\n",
    "                    # ...Stuff the full filepath in a list as a value in a dict\n",
    "                    # where the 5-char code is the key; this facilitates easy lookup later\n",
    "                    # If the key code isn't already in the dict, add and add the first full path\n",
    "                    # as first item in the list that is the val\n",
    "                    if not key_code in filepaths[k]:\n",
    "                        filepaths[k][key_code] = [p]\n",
    "\n",
    "                    # If the key code is already in the dict, append this file path\n",
    "                    # to list located at this key\n",
    "                    else:\n",
    "                        filepaths[k][key_code].append(p)\n",
    "\n",
    "\n",
    "    # Print All The Things to make sure our output is what we expect\n",
    "    print(f\"\\nOutput of get_filepaths (var filepaths):\")\n",
    "\n",
    "    for k, val in filepaths.items():\n",
    "        print(f\"\\tKey: {k}, Values:\")\n",
    "        for x, y in val.items():\n",
    "            print(f\"\\t\\tSubkey: {x}, Values:\")\n",
    "            for x in y:\n",
    "                print(f\"\\t\\t\\t{x}\")\n",
    "                  \n",
    "    return filepaths\n",
    "\n",
    "\n",
    "########## ########## ########## \n",
    "\n",
    "# Get a list of all field office server names...from somewhere\n",
    "def get_satellites(satellite_source):\n",
    "\n",
    "    # Test local flavor of this involves getting the list of folder names\n",
    "    # that correspond to the various field offices (e.g. _field_office_1)\n",
    "    if test_local:\n",
    "        \n",
    "        satellite_list = [f for f in os.listdir(satellite_source)]\n",
    "\n",
    "        # We are copying FROM mothership so exclude her\n",
    "        # from the list of copy TO dirs\n",
    "        m = os.path.basename(mothership)\n",
    "        if m in satellite_list:\n",
    "            satellite_list.remove(m)\n",
    "\n",
    "    # Assumes that for the Real Deal the satellite host names come from\n",
    "    # either a feature class or feature service URL\n",
    "    # TO DO...figure out why import of arcpy is not working,\n",
    "    # even if ArcGIS Pro is open etc.\n",
    "    else:\n",
    "\n",
    "        search_field = \"name_of_the_field\"\n",
    "\n",
    "        satellite_list = [row[0] for row in arcpy.da.SearchCursor(satellite_source, search_field)]\n",
    "\n",
    "        # We are copying FROM mothership so exclude her\n",
    "        # from the list of copy TO hosts\n",
    "        if mothership in satellite_list:\n",
    "            satellite_list.remove(mothership)\n",
    "\n",
    "    # Print All The Things to make sure our output is what we expect\n",
    "    print(\"\\nOutput of get_satellites (satellite_list):\")\n",
    "    for s in satellite_list:\n",
    "        print(f\"\\t{s}\")\n",
    "\n",
    "    return satellite_list\n",
    "\n",
    "\n",
    "\n",
    "########## ########## ########## \n",
    "\n",
    "def get_satellite_dirs(satellite):\n",
    "\n",
    "    mdb_dir = Path(get_root(satellite), \"FOTG\", \"Section_II\")\n",
    "\n",
    "    shp_dir = Path(get_root(satellite), \"geodata\", \"soils\")\n",
    "\n",
    "    satellite_dirs = {mdb: mdb_dir, shp: shp_dir}\n",
    "\n",
    "    print(f\"\\nOutput of get_satellite_dirs({satellite}):\")\n",
    "    for k, v in satellite_dirs.items():\n",
    "        print(f\"\\tKey: {k}, value: {v}\")\n",
    "\n",
    "    return satellite_dirs\n",
    "\n",
    "\n",
    "\n",
    "########## ########## ########## \n",
    "\n",
    "# Figure out which fo need which files\n",
    "def get_sat_required(satellite_dirs, sat):\n",
    "\n",
    "    sat_required = {}\n",
    "\n",
    "    for k, v in satellite_dirs.items():\n",
    "\n",
    "        current_files = [f for f in os.listdir(v) if f.endswith(k)]\n",
    "\n",
    "        if k == mdb:\n",
    "            filtered_list = [f for f in current_files if f.startswith(prefixes[mdb])]\n",
    "\n",
    "            sat_required[k] = set([str(f.split(prefixes[mdb])[1][:5]) for f in filtered_list])\n",
    "            \n",
    "        elif k == shp:\n",
    "            filtered_list = [f for f in current_files if f.startswith(prefixes[shp])]\n",
    "\n",
    "            sat_required[k] = set([str(f.split(prefixes[shp])[1][:5]) for f in filtered_list])\n",
    "\n",
    "    # Print All The Things to make sure our output is what we expect\n",
    "    print(f\"\\nOutput of get_sat_required {sat}:\")\n",
    "    for k, v in sat_required.items():\n",
    "        print(f\"\\tKey: {k}, value: {v}\")\n",
    "\n",
    "    return sat_required\n",
    "\n",
    "\n",
    "\n",
    "########## ########## ########## \n",
    "\n",
    "def archive_old(satellite_dirs, ext):\n",
    "    \n",
    "    # Construct the path to the \"Old to delete\" dir\n",
    "    # This is a subfolder within the dir\n",
    "    # to which we want to copy the new files\n",
    "    archive = Path(satellite_dirs[ext], \"Old to Delete\")\n",
    "\n",
    "    # If it exists (as it should), delete it and all files in it\n",
    "    if os.path.exists(archive):\n",
    "        shutil.rmtree(archive)\n",
    "\n",
    "    # Then remake it so it's a fresh folder w/no files in it\n",
    "    os.makedirs(archive, exist_ok=True)\n",
    "\n",
    "    # For all the files in the current directory\n",
    "    # (Do I need to narrow this down at all...?)\n",
    "    current_files = os.listdir(satellite_dirs[ext])\n",
    "\n",
    "    # Iterate through all the files currently in our working dir\n",
    "    for c in current_files:\n",
    "\n",
    "        # Construct the string for where the existing file\n",
    "        move_path = Path(satellite_dirs[ext], c)\n",
    "\n",
    "        # Construct the string for the future archived file\n",
    "        moved_path = Path(satellite_dirs[ext], archive, c)\n",
    "\n",
    "        # If it isn't a file (i.e. if it's a dir) don't touch it\n",
    "        if not os.path.isfile(move_path):\n",
    "            continue\n",
    "\n",
    "        # Otherwise move it to the archive\n",
    "        shutil.move(move_path, moved_path)\n",
    "\n",
    "\n",
    "\n",
    "########## ########## ########## \n",
    "\n",
    "def copy_file(in_file, to_dir):\n",
    "\n",
    "    #try:\n",
    "        #arcpy.management.Copy(in_file, out_file)\n",
    "\n",
    "    # YES I KNOW...will expand later\n",
    "    #except Exception as e:\n",
    "        #print(e)\n",
    "\n",
    "    try:\n",
    "        shutil.copy(in_file, to_dir)\n",
    "\n",
    "    # YES I KNOW...will expand later\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "\n",
    "########## ########## ########## \n",
    "\n",
    "# Iterate through all the  satellite field office servers,\n",
    "# generate lists of the specific files they need if applicable,\n",
    "# Then copy the appropriate files\n",
    "def iter_satellites(mothership_filepaths, satellite_list):\n",
    "\n",
    "    for sat in satellite_list:\n",
    "\n",
    "        # Rest of this script assumes we get the full server host name here\n",
    "        satellite_dirs = get_satellite_dirs(sat)\n",
    "\n",
    "        # Get the list of 5-char codes ALREADY IN the dirs...\n",
    "        sat_required = get_sat_required(satellite_dirs, sat)\n",
    "\n",
    "        # Iterate through the 5-char codes in the dest dir\n",
    "        for ext, code_list in sat_required.items():\n",
    "\n",
    "            # Archive any files currently in the destination dir\n",
    "            archive_old(satellite_dirs, ext)\n",
    "\n",
    "            # For each 5-char code\n",
    "            for code in code_list:\n",
    "\n",
    "                # For each individual file containing that 5-char code\n",
    "                # (This iters once for mdbs, but multiple times for shps)\n",
    "                for path in mothership_filepaths[ext][code]:\n",
    "\n",
    "                    # Do the thing\n",
    "                    copy_file(path, satellite_dirs[ext])\n",
    "\n",
    "\n",
    "\n",
    "########## ########## ########## ########## ########## ########## \n",
    "\n",
    "# DO THE THING\n",
    "# I.E. ALL THE CALLS\n",
    "\n",
    "mothership_dirs = get_mothership_dirs()\n",
    "\n",
    "consolidate_mdbs(mothership_dirs)\n",
    "\n",
    "mothership_filepaths = get_filepaths(mothership_dirs)\n",
    "\n",
    "satellite_list = get_satellites(satellite_source)\n",
    "\n",
    "iter_satellites(mothership_filepaths, satellite_list)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
